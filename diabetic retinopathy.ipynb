{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667c8c76-968a-403b-b998-b55a11734edb",
   "metadata": {},
   "source": [
    "### import librarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb4cbe6-f035-4027-8b83-1b436889bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7208ddfa-6b02-4368-af77-8c3083c8205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da97412-c2ef-41c3-80bf-467949ae479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposedNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, in_channels=3):\n",
    "        super().__init__()\n",
    "        # Stem\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBlock(in_channels, 32),  # 1\n",
    "            ConvBlock(32, 32),           # 2\n",
    "            nn.MaxPool2d(2)              # /2\n",
    "        )\n",
    "\n",
    "        # Main branch stacks\n",
    "        self.main = nn.Sequential(\n",
    "            ConvBlock(32, 64),           # 3\n",
    "            ConvBlock(64, 64),           # 4\n",
    "            nn.MaxPool2d(2),             # /4\n",
    "\n",
    "            ConvBlock(64, 128),          # 5\n",
    "            ConvBlock(128, 128),         # 6\n",
    "            nn.MaxPool2d(2),             # /8\n",
    "\n",
    "            ConvBlock(128, 256),         # 7\n",
    "            ConvBlock(256, 256),         # 8\n",
    "            # we'll merge with parallel branch here\n",
    "        )\n",
    "\n",
    "        # Parallel branch (shallower)\n",
    "        self.parallel = nn.Sequential(\n",
    "            ConvBlock(32, 48),           # p1\n",
    "            nn.MaxPool2d(2),             # /4\n",
    "            ConvBlock(48, 96),           # p2\n",
    "            nn.MaxPool2d(2),             # /8\n",
    "        )\n",
    "\n",
    "        # After concatenation\n",
    "        self.post_merge = nn.Sequential(\n",
    "            ConvBlock(256 + 96, 256),    # 9\n",
    "            ConvBlock(256, 256),         # 10\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        # classifier head: keep small to be lightweight\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.stem(x)\n",
    "        m = self.main(x0)\n",
    "        p = self.parallel(x0)\n",
    "        # resize if shapes mismatch (they should match spatial dims)\n",
    "        if m.shape[2:] != p.shape[2:]:\n",
    "            # adaptive pool to match\n",
    "            p = F.adaptive_avg_pool2d(p, m.shape[2:])\n",
    "        merged = torch.cat([m, p], dim=1)\n",
    "        out = self.post_merge(merged)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377d57cd-4f20-4f66-923f-9e0fad46c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05ec972-2811-4bac-91f7-5fa73f9d3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(stage: int, size: int = 227):\n",
    "    common = [\n",
    "        transforms.Resize((size, size)),\n",
    "    ]\n",
    "\n",
    "    # augmentation used during training\n",
    "    train_aug = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size, scale=(0.85, 1.15), ratio=(0.9, 1.1)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        # Paper uses zero-centered normalization. We'll apply 0-centered ~ map to [-1,1]\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    val_aug = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    return train_aug, val_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d44177-a4b3-439a-a8cd-db402ec021c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, loader=lambda x: Image.open(x).convert('RGB')):\n",
    "        import pandas as pd\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = self.root / row['image']\n",
    "        img = self.loader(path)\n",
    "        label = int(row['label'])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec1db6db-8098-495a-99f1-d9e2454868c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    pbar = tqdm(loader, leave=False)\n",
    "    for x, y in pbar:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        preds = torch.argmax(out.detach(), dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(y.cpu().numpy().tolist())\n",
    "        pbar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88db76aa-8658-4ca7-9093-37fb979bf70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            preds = torch.argmax(out, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(y.cpu().numpy().tolist())\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return epoch_loss, epoch_acc, kappa, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a71a310-5477-4a72-ba60-655adc08875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_finetune_head(model: nn.Module, num_classes: int):\n",
    "    # replace classifier head while keeping backbone frozen in transfer learning\n",
    "    if hasattr(model, 'classifier'):\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError('Model does not have classifier attribute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f70103b7-a30a-4dcf-b3f1-037d4c77e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_backbone(model: nn.Module):\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'classifier' not in name:\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bff807d5-01f1-40a3-a88d-fc1a3fc888b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_transform, val_transform = get_transforms(stage=args.stage, size=args.size)\n",
    "\n",
    "    # Data loaders - this script expects folder-per-class if no CSV is provided.\n",
    "    if args.csv:\n",
    "        train_ds = CSVDataset(args.csv, args.data_dir, transform=train_transform)\n",
    "        # For simplicity reuse same csv for val in this scaffold; in practice split\n",
    "        val_ds = CSVDataset(args.csv, args.data_dir, transform=val_transform)\n",
    "    else:\n",
    "        train_ds = datasets.ImageFolder(os.path.join(args.data_dir, 'train'), transform=train_transform)\n",
    "        val_ds = datasets.ImageFolder(os.path.join(args.data_dir, 'val'), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    num_classes = 2 if args.stage == 1 else args.num_classes\n",
    "\n",
    "    model = ProposedNet(num_classes=num_classes).to(device)\n",
    "    print(f\"Model params: {count_parameters(model)/1e6:.3f}M\")\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    best_kappa = -1.0\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f\"Epoch {epoch+1}/{args.epochs}\")\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, val_kappa, cm = eval_epoch(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"train loss {train_loss:.4f} acc {train_acc:.4f}\")\n",
    "        print(f\"val   loss {val_loss:.4f} acc {val_acc:.4f} kappa {val_kappa:.4f}\")\n",
    "        print(\"confusion matrix:\\n\", cm)\n",
    "\n",
    "        # checkpoint\n",
    "        if val_kappa > best_kappa:\n",
    "            best_kappa = val_kappa\n",
    "            torch.save({'model_state': model.state_dict(), 'epoch': epoch}, args.checkpoint)\n",
    "            print(f\"Saved best model to {args.checkpoint}\")\n",
    "\n",
    "    print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ae74ccf-f4f9-4c54-8fce-acec5f06f572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data-dir DATA_DIR] [--csv CSV]\n",
      "                             [--stage STAGE] [--num-classes NUM_CLASSES]\n",
      "                             [--epochs EPOCHS] [--batch-size BATCH_SIZE]\n",
      "                             [--lr LR] [--size SIZE] [--checkpoint CHECKPOINT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/mahabur-alam/.local/share/jupyter/runtime/kernel-420a5f88-4c40-4ebf-9552-799ca3421b40.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args([])\n\u001b[0;32m---> 19\u001b[0m main(args)\n",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     10\u001b[0m     val_ds \u001b[38;5;241m=\u001b[39m CSVDataset(args\u001b[38;5;241m.\u001b[39mcsv, args\u001b[38;5;241m.\u001b[39mdata_dir, transform\u001b[38;5;241m=\u001b[39mval_transform)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     train_ds \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m), transform\u001b[38;5;241m=\u001b[39mtrain_transform)\n\u001b[1;32m     13\u001b[0m     val_ds \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m), transform\u001b[38;5;241m=\u001b[39mval_transform)\n\u001b[1;32m     15\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    329\u001b[0m         root,\n\u001b[1;32m    330\u001b[0m         loader,\n\u001b[1;32m    331\u001b[0m         IMG_EXTENSIONS \u001b[38;5;28;01mif\u001b[39;00m is_valid_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    332\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[1;32m    333\u001b[0m         target_transform\u001b[38;5;241m=\u001b[39mtarget_transform,\n\u001b[1;32m    334\u001b[0m         is_valid_file\u001b[38;5;241m=\u001b[39mis_valid_file,\n\u001b[1;32m    335\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    336\u001b[0m     )\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/train'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data-dir', type=str, default='./data', help='path to dataset root (train/ val folders) or root for CSV loader')\n",
    "    parser.add_argument('--csv', type=str, default=None, help='optional csv file mapping images to labels')\n",
    "    parser.add_argument('--stage', type=int, default=1, help='1 for DR detection (binary), 2 for severity (multi-class)')\n",
    "    parser.add_argument('--num-classes', type=int, default=5, help='number of classes for severity task')\n",
    "    parser.add_argument('--epochs', type=int, default=100)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--size', type=int, default=227)\n",
    "    parser.add_argument('--checkpoint', type=str, default='best_model.pt')\n",
    "\n",
    "    # For Jupyter/IPython, allow args to be ignored\n",
    "    try:\n",
    "        args = parser.parse_args()\n",
    "    except SystemExit:\n",
    "        args = parser.parse_args([])\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f07728-e44e-4c5a-bb1f-ce6e80f6b3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
